{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho modificato la struttuta dei manuali per evitare i conflitti. In particolare nei moduli del treno arancio e viola nella parte della base anteriore\n",
    "#ho elimitato la frase assemblaggio finale poichè per eliminare il problema della presenza costante dell'assemblaggio finale nella risposta, in questo codice\n",
    "#c'è una funzione che elimina la parte di testo che contiene \"assemblaggio finale\" dalle risposte del chatbot o a meno che non vengia chiesto specificatamente dall'autente.\n",
    "#Questa funzione però non permetteva al cahtbot di dare istruzioni riguardo la base anteriore perchè nel manuale, in quella sezione, copariva la frase assemblaggio finale.\n",
    "#In più ho modificato l'interfaccia perchè con l'altro codice le frasi troppo lunge le troncava con dei puntini di sospensione, in questo modo la risposta si legge completamente.\n",
    "#NB:quando l'utente pone la domanda il chatbot impiega qualche secondo a restituire la risposta.\n",
    "#(NO)\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di GPT-2\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_gpt2(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        \n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        # Ricerca nei manuali\n",
    "        response_from_manuals = self._search_in_manuals(product, query)\n",
    "        \n",
    "        # Usa GPT-2 per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            gpt_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            gpt_response = query_gpt2(gpt_prompt)\n",
    "        \n",
    "            if 'generated_text' in gpt_response:\n",
    "                return response_from_manuals + [self._filter_gpt_response(gpt_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _search_in_manuals(self, product, query):\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_gpt_response(self, response, query):\n",
    "        # Includi la sezione \"assemblaggio finale\" solo se esplicitamente richiesta\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#risponde sia alle fasi: poiche ad ogni sezione è stata associata una fase in ordine crescente sia se si chiede nello specifico il modulo.\n",
    "#devono essere effettuate delle richieste che contengano fase 1, fase 2 ecc o il nome del modulo.\n",
    "#Risponde se gli cheidi fase 1 e fase 2. Se gli chied fase 1 e 2 risponde solo con le istruzioni della fase 1.\n",
    "#(NO)\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di GPT-2\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_gpt2(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Verifica se la query riguarda una fase o una sezione specifica\n",
    "        if self._is_phase_query(query):\n",
    "            response_from_manuals = self._search_phase(product, query)\n",
    "        else:\n",
    "            response_from_manuals = self._search_section(product, query)\n",
    "        \n",
    "        # Usa GPT-2 per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            gpt_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            gpt_response = query_gpt2(gpt_prompt)\n",
    "        \n",
    "            if 'generated_text' in gpt_response:\n",
    "                return response_from_manuals + [self._filter_gpt_response(gpt_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_gpt_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta GPT-2, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codice con Falcon-GPT3 (NO)\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Verifica se la query riguarda una fase o una sezione specifica\n",
    "        if self._is_phase_query(query):\n",
    "            response_from_manuals = self._search_phase(product, query)\n",
    "        else:\n",
    "            response_from_manuals = self._search_section(product, query)\n",
    "        \n",
    "        # Usa Falcon per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "        \n",
    "            if 'generated_text' in falcon_response:\n",
    "                return response_from_manuals + [self._filter_falcon_response(falcon_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZZA FALCON, RISPONDE UN PO' PIU VELOCEMENTE E RISPONDE A DOMANDE COMBINATE (NO)\n",
    "#risponde sia alle fasi: poiche ad ogni sezione è stata associata una fase in ordine crescente sia se si chiede nello specifico il modulo.\n",
    "#devono essere effettuate delle richieste che contengano fase 1, fase 2 ecc o il nome del modulo.\n",
    "#Risponde se gli cheidi fase 1 e fase 2. Se gli chied fase 1 e 2 risponde solo con le istruzioni della fase 1.\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Suddividi la query in possibili sezioni (es. \"fase 1\" e \"base anteriore\")\n",
    "        queries = query.split(' e ')  # Dividi la query per \"e\" o altre parole chiave che collegano più richieste\n",
    "        \n",
    "        responses = []\n",
    "        \n",
    "        for q in queries:\n",
    "            q = q.strip()  # Rimuovi spazi bianchi\n",
    "\n",
    "            # Verifica se la query riguarda una fase o una sezione specifica\n",
    "            if self._is_phase_query(q):\n",
    "                response_from_manuals = self._search_phase(product, q)\n",
    "            else:\n",
    "                response_from_manuals = self._search_section(product, q)\n",
    "            \n",
    "            # Usa Falcon per arricchire la risposta\n",
    "            if response_from_manuals:\n",
    "                falcon_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {q}\"\n",
    "                falcon_response = query_falcon(falcon_prompt)\n",
    "            \n",
    "                if 'generated_text' in falcon_response:\n",
    "                    responses.append(response_from_manuals + [self._filter_falcon_response(falcon_response['generated_text'].strip(), q)])\n",
    "                else:\n",
    "                    responses.append(response_from_manuals)\n",
    "            else:\n",
    "                responses.append(response_from_manuals)\n",
    "        \n",
    "        return [item for sublist in responses for item in sublist]  # Appiattisci le risposte per concatenarle\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7bd1f27a524a2685ee0d665c9c2039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Prodotto:', options=(('Seleziona un prodotto', None), ('Treno Arancio', 'Treno Arancio')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac92606d02b46889c0ed513e4d10d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='AssemblyBot:', placeholder='Cosa vuoi fare?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3db2104242480f8418936fb95ae45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Invia', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66361f77ac264071b0e56293a3426c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Conversazione:', layout=Layout(height='400px', width='100%'), placeholder='', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#UTILIZZA FALCON, RISPONDE UN PO' PIU VELOCEMENTE E RISPONDE A DOMANDE COMBINATE (DA UTILIZZARE)\n",
    "#Non risponde a domande tipo quale è il primo passo da fare, quale è la prima cosa da fare. Per fare questo bisognerebbe utilizzare modelli più perfprmanti\n",
    "#o si richiede l'utilizzo di Pytorch, tensorflow che io non riesco proprio ad utilizzare sul mio pc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica utilizzando solo i numeri delle fasi.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca solo i numeri delle fasi nel testo\n",
    "            phase_match = re.search(r'\\b(\\d+)\\b', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip()\n",
    "                if phase_number not in self.sections[product]:\n",
    "                    self.sections[product][phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Suddividi la query in possibili sezioni (es. \"fase 1\" e \"base anteriore\")\n",
    "        queries = [q.strip() for q in re.split(r'\\s+e\\s+|\\s+and\\s+', query, flags=re.IGNORECASE)]\n",
    "        \n",
    "        # Mappa delle risposte per mantenere l'ordine\n",
    "        responses_map = {}\n",
    "        general_responses = []\n",
    "\n",
    "        for q in queries:\n",
    "            # Verifica se la query riguarda una fase\n",
    "            if self._is_phase_query(q):\n",
    "                phase_response = self._search_phase(product, q)\n",
    "                if phase_response:\n",
    "                    responses_map[q] = phase_response\n",
    "            else:\n",
    "                section_response = self._search_section(product, q)\n",
    "                if section_response:\n",
    "                    general_responses.append((q, section_response))\n",
    "        \n",
    "        # Unisci e ordina le risposte\n",
    "        ordered_responses = []\n",
    "        for q in queries:\n",
    "            if q in responses_map:\n",
    "                ordered_responses.extend(responses_map[q])\n",
    "            elif any(q.lower() in item[0].lower() for item in general_responses):\n",
    "                for item in general_responses:\n",
    "                    if q.lower() in item[0].lower():\n",
    "                        ordered_responses.extend(item[1])\n",
    "        \n",
    "        # Usa Falcon per arricchire la risposta\n",
    "        combined_response = ordered_responses\n",
    "        if combined_response:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(combined_response)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "        \n",
    "            if 'generated_text' in falcon_response:\n",
    "                return combined_response + [self._filter_falcon_response(falcon_response['generated_text'].strip(), query)]\n",
    "            else:\n",
    "                return combined_response\n",
    "        \n",
    "        return combined_response if combined_response else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale utilizzando solo numeri.\"\"\"\n",
    "        return bool(re.search(r'\\b\\d+\\b', query))\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Cosa vuoi fare?',\n",
    "    description='AssemblyBot:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
