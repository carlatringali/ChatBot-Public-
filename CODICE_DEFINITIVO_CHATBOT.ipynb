{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RISPONDE SEMPRE A FASE PRECEDENTE E SUCCESSIVA (DA UTILIZZARE)\n",
    "#Non risponde a domande tipo:\"quale è il primo passo?\", \"quale è la prima cosa da fare?\", per chiedere una fase specifica va chiesta \"(frase) fase 1\"\n",
    "#E' un chatbot RAG che utilizza un linguaggio LLM per comprendere meglio il contesto, la domanda. In questi contesti, dove l'assistente deve prendere informazioni da manuali sono\n",
    "#più adatti i chatbot RAG, mentre gli LLM per assistenti che devono rispondere a domande generiche e quindi hanno bisogno di essere addestrati con più informazioni.\n",
    "\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "        self.current_phase = {}  # Memorizza la fase corrente per ciascun prodotto\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            phase_match = re.search(r'\\b(\\d+)\\b', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip()\n",
    "                if phase_number not in self.sections[product]:\n",
    "                    self.sections[product][phase_number] = (i, chunk)\n",
    "\n",
    "    def _get_current_phase(self, product):\n",
    "        \"\"\"Restituisce la fase corrente del prodotto.\"\"\"\n",
    "        return self.current_phase.get(product)\n",
    "\n",
    "    def _set_current_phase(self, product, phase):\n",
    "        \"\"\"Imposta la fase corrente per il prodotto.\"\"\"\n",
    "        self.current_phase[product] = phase\n",
    "\n",
    "    def _get_next_phase(self, product):\n",
    "        \"\"\"Trova la fase successiva.\"\"\"\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        if current_phase is None:\n",
    "            return [\"Non è stata identificata una fase corrente.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: int(x[0]))\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if int(phase) == current_phase + 1:\n",
    "                self._set_current_phase(product, int(phase))  # Aggiorna la fase corrente\n",
    "                return [content]\n",
    "        \n",
    "        return [\"Non ci sono fasi successive disponibili.\"]\n",
    "\n",
    "    def _get_previous_phase(self, product):\n",
    "        \"\"\"Trova la fase precedente.\"\"\"\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        if current_phase is None:\n",
    "            return [\"Non è stata identificata una fase corrente.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: int(x[0]))\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if int(phase) == current_phase - 1:\n",
    "                self._set_current_phase(product, int(phase))  # Aggiorna la fase corrente\n",
    "                return [content]\n",
    "        \n",
    "        return [\"Non ci sono fasi precedenti disponibili.\"]\n",
    "\n",
    "    def _update_current_phase_from_chunk(self, product, chunk):\n",
    "        \"\"\"Aggiorna la fase corrente in base all'indice del chunk.\"\"\"\n",
    "        for phase, (index, content) in self.sections[product].items():\n",
    "            if content == chunk:\n",
    "                self._set_current_phase(product, int(phase))\n",
    "                break\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        self._associate_phases(product)\n",
    "\n",
    "        if \"fase successiva\" in query.lower():\n",
    "            phase_response = self._get_next_phase(product)\n",
    "            return phase_response\n",
    "\n",
    "        if \"fase precedente\" in query.lower():\n",
    "            phase_response = self._get_previous_phase(product)\n",
    "            return phase_response\n",
    "\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        phase_number_in_query = self._get_current_phase_from_query(query)\n",
    "\n",
    "        if phase_number_in_query is not None:\n",
    "            self._set_current_phase(product, phase_number_in_query)\n",
    "\n",
    "        responses_map = {}\n",
    "        general_responses = []\n",
    "\n",
    "        queries = [q.strip() for q in re.split(r'\\s+e\\s+|\\s+and\\s+', query, flags=re.IGNORECASE)]\n",
    "        for q in queries:\n",
    "            if self._is_phase_query(q):\n",
    "                phase_response = self._search_phase(product, q)\n",
    "                if phase_response:\n",
    "                    responses_map[q] = phase_response\n",
    "            else:\n",
    "                section_response = self._search_section(product, q)\n",
    "                if section_response:\n",
    "                    general_responses.append((q, section_response))\n",
    "\n",
    "        ordered_responses = []\n",
    "        for q in queries:\n",
    "            if q in responses_map:\n",
    "                ordered_responses.extend(responses_map[q])\n",
    "            elif any(q.lower() in item[0].lower() for item in general_responses):\n",
    "                for item in general_responses:\n",
    "                    if q.lower() in item[0].lower():\n",
    "                        ordered_responses.extend(item[1])\n",
    "\n",
    "        combined_response = ordered_responses\n",
    "        if combined_response:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(combined_response)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "\n",
    "            if 'generated_text' in falcon_response:\n",
    "                response_text = falcon_response['generated_text'].strip()\n",
    "                self._update_current_phase_from_chunk(product, combined_response[-1])  # Aggiorna la fase corrente\n",
    "                return combined_response + [self._filter_falcon_response(response_text, query)]\n",
    "            else:\n",
    "                return combined_response\n",
    "\n",
    "        return combined_response if combined_response else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _get_current_phase_from_query(self, query):\n",
    "        \"\"\"Estrae la fase corrente dalla query.\"\"\"\n",
    "        match = re.search(r'\\b\\d+\\b', query)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return None\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale utilizzando solo numeri.\"\"\"\n",
    "        return bool(re.search(r'\\b\\d+\\b', query))\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        if results:\n",
    "            self._update_current_phase_from_chunk(product, results[-1])  # Aggiorna la fase corrente\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        if results:\n",
    "            self._update_current_phase_from_chunk(product, results[-1])  # Aggiorna la fase corrente\n",
    "        \n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response\n",
    "        \n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Cosa vuoi fare?',\n",
    "    description='AssemblyBot:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(lambda _: on_submit())\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare solo la risposta corrente)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_submit():\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        \n",
    "        # Unisci gli elementi della lista di risposte in una singola stringa\n",
    "        response_text = \"\\n\".join(response)\n",
    "        \n",
    "        # Mostra solo la risposta corrente, non le precedenti\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\nBot: {response_text}\\n\"\n",
    "\n",
    "        query_input.value = ''\n",
    "\n",
    "\n",
    "# Layout dell'interfaccia\n",
    "display(widgets.VBox([product_dropdown, query_input, submit_button, conversation_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ho modificato la struttuta dei manuali per evitare i conflitti. In particolare nei moduli del treno arancio e viola nella parte della base anteriore\n",
    "#ho elimitato la frase assemblaggio finale poichè per eliminare il problema della presenza costante dell'assemblaggio finale nella risposta, in questo codice\n",
    "#c'è una funzione che elimina la parte di testo che contiene \"assemblaggio finale\" dalle risposte del chatbot o a meno che non vengia chiesto specificatamente dall'autente.\n",
    "#Questa funzione però non permetteva al cahtbot di dare istruzioni riguardo la base anteriore perchè nel manuale, in quella sezione, copariva la frase assemblaggio finale.\n",
    "#In più ho modificato l'interfaccia perchè con l'altro codice le frasi troppo lunge le troncava con dei puntini di sospensione, in questo modo la risposta si legge completamente.\n",
    "#NB:quando l'utente pone la domanda il chatbot impiega qualche secondo a restituire la risposta.\n",
    "#(NON UTILIZZARE)\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di GPT-2\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_gpt2(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        \n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        # Ricerca nei manuali\n",
    "        response_from_manuals = self._search_in_manuals(product, query)\n",
    "        \n",
    "        # Usa GPT-2 per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            gpt_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            gpt_response = query_gpt2(gpt_prompt)\n",
    "        \n",
    "            if 'generated_text' in gpt_response:\n",
    "                return response_from_manuals + [self._filter_gpt_response(gpt_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _search_in_manuals(self, product, query):\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_gpt_response(self, response, query):\n",
    "        # Includi la sezione \"assemblaggio finale\" solo se esplicitamente richiesta\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#risponde sia alle fasi: poiche ad ogni sezione è stata associata una fase in ordine crescente sia se si chiede nello specifico il modulo.\n",
    "#devono essere effettuate delle richieste che contengano fase 1, fase 2 ecc o il nome del modulo.\n",
    "#Risponde se gli cheidi fase 1 e fase 2. Se gli chied fase 1 e 2 risponde solo con le istruzioni della fase 1.\n",
    "#(NON UTILIZZARE)\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di GPT-2\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_gpt2(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Verifica se la query riguarda una fase o una sezione specifica\n",
    "        if self._is_phase_query(query):\n",
    "            response_from_manuals = self._search_phase(product, query)\n",
    "        else:\n",
    "            response_from_manuals = self._search_section(product, query)\n",
    "        \n",
    "        # Usa GPT-2 per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            gpt_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            gpt_response = query_gpt2(gpt_prompt)\n",
    "        \n",
    "            if 'generated_text' in gpt_response:\n",
    "                return response_from_manuals + [self._filter_gpt_response(gpt_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_gpt_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta GPT-2, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codice con Falcon-GPT3 (NON UTILIZZARE)\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Verifica se la query riguarda una fase o una sezione specifica\n",
    "        if self._is_phase_query(query):\n",
    "            response_from_manuals = self._search_phase(product, query)\n",
    "        else:\n",
    "            response_from_manuals = self._search_section(product, query)\n",
    "        \n",
    "        # Usa Falcon per arricchire la risposta\n",
    "        if response_from_manuals:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "        \n",
    "            if 'generated_text' in falcon_response:\n",
    "                return response_from_manuals + [self._filter_falcon_response(falcon_response['generated_text'].strip(), query)]\n",
    "        \n",
    "        return response_from_manuals\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZZA FALCON, RISPONDE UN PO' PIU VELOCEMENTE E RISPONDE A DOMANDE COMBINATE (NON RISPONDE ALLE DOMANDE)\n",
    "#risponde sia alle fasi: poiche ad ogni sezione è stata associata una fase in ordine crescente sia se si chiede nello specifico il modulo.\n",
    "#devono essere effettuate delle richieste che contengano fase 1, fase 2 ecc o il nome del modulo.\n",
    "#Risponde se gli cheidi fase 1 e fase 2. Se gli chied fase 1 e 2 risponde solo con le istruzioni della fase 1.\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica e assegna una priorità in base all'ordine.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca la fase nel testo\n",
    "            phase_match = re.search(r'fase\\s*\\d+', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip().lower()\n",
    "                self.sections[product][phase_number] = (i, chunk)  # Associa il numero della fase e l'ordine\n",
    "            else:\n",
    "                # Associa una fase implicita basata sull'ordine\n",
    "                if i > 0:\n",
    "                    prev_phase_number = f\"fase {i + 1}\"  # Corregge il numero di fase per essere 1-based\n",
    "                    self.sections[product][prev_phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Suddividi la query in possibili sezioni (es. \"fase 1\" e \"base anteriore\")\n",
    "        queries = query.split(' e ')  # Dividi la query per \"e\" o altre parole chiave che collegano più richieste\n",
    "        \n",
    "        responses = []\n",
    "        \n",
    "        for q in queries:\n",
    "            q = q.strip()  # Rimuovi spazi bianchi\n",
    "\n",
    "            # Verifica se la query riguarda una fase o una sezione specifica\n",
    "            if self._is_phase_query(q):\n",
    "                response_from_manuals = self._search_phase(product, q)\n",
    "            else:\n",
    "                response_from_manuals = self._search_section(product, q)\n",
    "            \n",
    "            # Usa Falcon per arricchire la risposta\n",
    "            if response_from_manuals:\n",
    "                falcon_prompt = f\"Given the following information from the manual: {', '.join(response_from_manuals)}, provide a precise answer to the question: {q}\"\n",
    "                falcon_response = query_falcon(falcon_prompt)\n",
    "            \n",
    "                if 'generated_text' in falcon_response:\n",
    "                    responses.append(response_from_manuals + [self._filter_falcon_response(falcon_response['generated_text'].strip(), q)])\n",
    "                else:\n",
    "                    responses.append(response_from_manuals)\n",
    "            else:\n",
    "                responses.append(response_from_manuals)\n",
    "        \n",
    "        return [item for sublist in responses for item in sublist]  # Appiattisci le risposte per concatenarle\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale.\"\"\"\n",
    "        return any(kw in query.lower() for kw in ['fase', 'fase finale', 'assemblaggio finale'])\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Inserisci la tua domanda',\n",
    "    description='Domanda:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Le risposte verranno visualizzate qui...',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZZA FALCON, RISPONDE UN PO' PIU VELOCEMENTE E RISPONDE A DOMANDE COMBINATE, ma non risponde a fase successiva e precedente  (IL CODICE NELL'ULTIMA CELLA SI)\n",
    "#Non risponde a domande tipo quale è il primo passo da fare, quale è la prima cosa da fare. Per fare questo bisognerebbe utilizzare modelli più perfprmanti\n",
    "#o si richiede l'utilizzo di Pytorch, tensorflow che io non riesco proprio ad utilizzare sul mio pc.\n",
    "#(NON UTILIZZARE)\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        \"\"\"Associa ogni chunk del manuale a una fase specifica utilizzando solo i numeri delle fasi.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Cerca solo i numeri delle fasi nel testo\n",
    "            phase_match = re.search(r'\\b(\\d+)\\b', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip()\n",
    "                if phase_number not in self.sections[product]:\n",
    "                    self.sections[product][phase_number] = (i, chunk)\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        # Associa le fasi al manuale\n",
    "        self._associate_phases(product)\n",
    "\n",
    "        # Suddividi la query in possibili sezioni (es. \"fase 1\" e \"base anteriore\")\n",
    "        queries = [q.strip() for q in re.split(r'\\s+e\\s+|\\s+and\\s+', query, flags=re.IGNORECASE)]\n",
    "        \n",
    "        # Mappa delle risposte per mantenere l'ordine\n",
    "        responses_map = {}\n",
    "        general_responses = []\n",
    "\n",
    "        for q in queries:\n",
    "            # Verifica se la query riguarda una fase\n",
    "            if self._is_phase_query(q):\n",
    "                phase_response = self._search_phase(product, q)\n",
    "                if phase_response:\n",
    "                    responses_map[q] = phase_response\n",
    "            else:\n",
    "                section_response = self._search_section(product, q)\n",
    "                if section_response:\n",
    "                    general_responses.append((q, section_response))\n",
    "        \n",
    "        # Unisci e ordina le risposte\n",
    "        ordered_responses = []\n",
    "        for q in queries:\n",
    "            if q in responses_map:\n",
    "                ordered_responses.extend(responses_map[q])\n",
    "            elif any(q.lower() in item[0].lower() for item in general_responses):\n",
    "                for item in general_responses:\n",
    "                    if q.lower() in item[0].lower():\n",
    "                        ordered_responses.extend(item[1])\n",
    "        \n",
    "        # Usa Falcon per arricchire la risposta\n",
    "        combined_response = ordered_responses\n",
    "        if combined_response:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(combined_response)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "        \n",
    "            if 'generated_text' in falcon_response:\n",
    "                return combined_response + [self._filter_falcon_response(falcon_response['generated_text'].strip(), query)]\n",
    "            else:\n",
    "                return combined_response\n",
    "        \n",
    "        return combined_response if combined_response else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale utilizzando solo numeri.\"\"\"\n",
    "        return bool(re.search(r'\\b\\d+\\b', query))\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        \"\"\"Cerca la fase richiesta nel manuale.\"\"\"\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        # Ordina le sezioni per fase e ordine cronologico\n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        \"\"\"Cerca una sezione specifica del manuale basata sulla query.\"\"\"\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        # Filtra la sezione \"assemblaggio finale\" se non richiesta esplicitamente\n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        # Trova i chunk più simili alla query\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        # Riduci la soglia per trovare più risposte\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        # Riduci la lunghezza delle risposte\n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        \"\"\"Filtro della risposta Falcon, mantenendo solo le informazioni rilevanti.\"\"\"\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response  # Mantieni l'intera risposta se richiesta\n",
    "        \n",
    "        # Altrimenti, filtra la sezione\n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "def on_submit(_):\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\n\" + \"\\n\\n\".join([f\"Bot: {info}\" for info in response])\n",
    "        query_input.value = \"\"\n",
    "    else:\n",
    "        conversation_output.value = \"Bot: Seleziona un prodotto e inserisci una domanda.\"\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Cosa vuoi fare?',\n",
    "    description='AssemblyBot:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare tutto il testo)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualizza tutti i widget\n",
    "display(product_dropdown, query_input, submit_button, conversation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffec9299363c4c7f8b68c1b7f600c3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Prodotto:', options=(('Seleziona un prodotto', None), ('Treno Arancio', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RISPONDE A FASE PRECEDENTE E SUCCESSIVA\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Configura l'API di Falcon\n",
    "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query_falcon(prompt):\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"error\": \"Request failed with status code \" + str(response.status_code)}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "class ManualLoader:\n",
    "    def __init__(self, manual_paths):\n",
    "        self.manuals = {}\n",
    "        self.load_manuals(manual_paths)\n",
    "\n",
    "    def load_manuals(self, paths):\n",
    "        for name, path in paths.items():\n",
    "            with open(path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                self.manuals[name] = self._split_into_chunks(content)\n",
    "\n",
    "    def _split_into_chunks(self, text):\n",
    "        chunks = [preprocess_text(chunk.strip()) for chunk in text.split('---') if chunk.strip()]\n",
    "        return chunks\n",
    "\n",
    "class AssemblyAssistantBot:\n",
    "    def __init__(self, manuals):\n",
    "        self.manual_loader = ManualLoader(manuals)\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self._fit_vectorizer()\n",
    "        self.sections = {}\n",
    "        self.current_phase = {}  # Memorizza la fase corrente per ciascun prodotto\n",
    "\n",
    "    def _fit_vectorizer(self):\n",
    "        all_chunks = []\n",
    "        for chunks in self.manual_loader.manuals.values():\n",
    "            all_chunks.extend(chunks)\n",
    "        self.vectorizer.fit(all_chunks)\n",
    "\n",
    "    def _associate_phases(self, product):\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        self.sections[product] = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            phase_match = re.search(r'\\b(\\d+)\\b', chunk)\n",
    "            if phase_match:\n",
    "                phase_number = phase_match.group().strip()\n",
    "                if phase_number not in self.sections[product]:\n",
    "                    self.sections[product][phase_number] = (i, chunk)\n",
    "\n",
    "    def _get_current_phase(self, product):\n",
    "        \"\"\"Restituisce la fase corrente del prodotto.\"\"\"\n",
    "        return self.current_phase.get(product)\n",
    "\n",
    "    def _set_current_phase(self, product, phase):\n",
    "        \"\"\"Imposta la fase corrente per il prodotto.\"\"\"\n",
    "        self.current_phase[product] = phase\n",
    "\n",
    "    def _get_next_phase(self, product):\n",
    "        \"\"\"Trova la fase successiva.\"\"\"\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        if current_phase is None:\n",
    "            return [\"Non è stata identificata una fase corrente.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: int(x[0]))\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if int(phase) == current_phase + 1:\n",
    "                self._set_current_phase(product, int(phase))  # Aggiorna la fase corrente\n",
    "                return [content]\n",
    "        \n",
    "        return [\"Non ci sono fasi successive disponibili.\"]\n",
    "\n",
    "    def _get_previous_phase(self, product):\n",
    "        \"\"\"Trova la fase precedente.\"\"\"\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        if current_phase is None:\n",
    "            return [\"Non è stata identificata una fase corrente.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: int(x[0]))\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if int(phase) == current_phase - 1:\n",
    "                self._set_current_phase(product, int(phase))  # Aggiorna la fase corrente\n",
    "                return [content]\n",
    "        \n",
    "        return [\"Non ci sono fasi precedenti disponibili.\"]\n",
    "\n",
    "    def handle_query(self, product, query):\n",
    "        if product not in self.manual_loader.manuals:\n",
    "            return [\"Manuale non trovato.\"]\n",
    "        \n",
    "        self._associate_phases(product)\n",
    "\n",
    "        if \"fase successiva\" in query.lower():\n",
    "            phase_response = self._get_next_phase(product)\n",
    "            return phase_response\n",
    "\n",
    "        if \"fase precedente\" in query.lower():\n",
    "            phase_response = self._get_previous_phase(product)\n",
    "            return phase_response\n",
    "\n",
    "        current_phase = self._get_current_phase(product)\n",
    "        phase_number_in_query = self._get_current_phase_from_query(query)\n",
    "\n",
    "        if phase_number_in_query is not None:\n",
    "            self._set_current_phase(product, phase_number_in_query)\n",
    "\n",
    "        responses_map = {}\n",
    "        general_responses = []\n",
    "\n",
    "        queries = [q.strip() for q in re.split(r'\\s+e\\s+|\\s+and\\s+', query, flags=re.IGNORECASE)]\n",
    "        for q in queries:\n",
    "            if self._is_phase_query(q):\n",
    "                phase_response = self._search_phase(product, q)\n",
    "                if phase_response:\n",
    "                    responses_map[q] = phase_response\n",
    "            else:\n",
    "                section_response = self._search_section(product, q)\n",
    "                if section_response:\n",
    "                    general_responses.append((q, section_response))\n",
    "\n",
    "        ordered_responses = []\n",
    "        for q in queries:\n",
    "            if q in responses_map:\n",
    "                ordered_responses.extend(responses_map[q])\n",
    "            elif any(q.lower() in item[0].lower() for item in general_responses):\n",
    "                for item in general_responses:\n",
    "                    if q.lower() in item[0].lower():\n",
    "                        ordered_responses.extend(item[1])\n",
    "\n",
    "        combined_response = ordered_responses\n",
    "        if combined_response:\n",
    "            falcon_prompt = f\"Given the following information from the manual: {', '.join(combined_response)}, provide a precise answer to the question: {query}\"\n",
    "            falcon_response = query_falcon(falcon_prompt)\n",
    "\n",
    "            if 'generated_text' in falcon_response:\n",
    "                return combined_response + [self._filter_falcon_response(falcon_response['generated_text'].strip(), query)]\n",
    "            else:\n",
    "                return combined_response\n",
    "\n",
    "        return combined_response if combined_response else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _get_current_phase_from_query(self, query):\n",
    "        \"\"\"Estrae la fase corrente dalla query.\"\"\"\n",
    "        match = re.search(r'\\b\\d+\\b', query)\n",
    "        if match:\n",
    "            return int(match.group())\n",
    "        return None\n",
    "\n",
    "    def _is_phase_query(self, query):\n",
    "        \"\"\"Determina se la query riguarda una fase del manuale utilizzando solo numeri.\"\"\"\n",
    "        return bool(re.search(r'\\b\\d+\\b', query))\n",
    "\n",
    "    def _search_phase(self, product, query):\n",
    "        query = preprocess_text(query)\n",
    "        if product not in self.sections:\n",
    "            return [\"Sezione del manuale non trovata.\"]\n",
    "        \n",
    "        sorted_phases = sorted(self.sections[product].items(), key=lambda x: x[1][0])\n",
    "\n",
    "        results = []\n",
    "        for phase, (order, content) in sorted_phases:\n",
    "            if phase in query:\n",
    "                results.append(content)\n",
    "        \n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata per la fase richiesta.\"]\n",
    "\n",
    "    def _search_section(self, product, query):\n",
    "        chunks = self.manual_loader.manuals.get(product, [])\n",
    "        if not chunks:\n",
    "            return [\"Nessuna informazione trovata nel manuale per questo prodotto.\"]\n",
    "        \n",
    "        include_final_assembly = any(kw in query.lower() for kw in ['assemblaggio finale', 'fase finale', 'finale'])\n",
    "        if not include_final_assembly:\n",
    "            chunks = [chunk for chunk in chunks if 'assemblaggio finale' not in chunk.lower()]\n",
    "        \n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        chunk_vecs = self.vectorizer.transform(chunks)\n",
    "        similarities = cosine_similarity(query_vec, chunk_vecs).flatten()\n",
    "\n",
    "        results = [chunks[index] for index, similarity in enumerate(similarities) if similarity > 0.1]\n",
    "        \n",
    "        results = self._truncate_responses(results)\n",
    "        \n",
    "        return results if results else [\"Nessuna informazione trovata.\"]\n",
    "\n",
    "    def _filter_falcon_response(self, response, query):\n",
    "        if 'assemblaggio finale' in query.lower():\n",
    "            return response\n",
    "        \n",
    "        pattern = re.compile(r'assemblaggio finale dei moduli.*', re.DOTALL)\n",
    "        filtered_response = pattern.split(response)[0].strip()\n",
    "        \n",
    "        return filtered_response\n",
    "\n",
    "    def _truncate_responses(self, responses, max_length=2000):\n",
    "        truncated_responses = []\n",
    "        for response in responses:\n",
    "            if len(response) > max_length:\n",
    "                truncated_responses.append(response[:max_length] + '...')\n",
    "            else:\n",
    "                truncated_responses.append(response)\n",
    "        return truncated_responses\n",
    "\n",
    "# Percorsi ai manuali\n",
    "manuals = {\n",
    "    \"Treno Arancio\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoArancio.txt\",\n",
    "    \"Treno Viola\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\TrenoViola.txt\",\n",
    "    \"Giraffa\": \"C:\\\\Users\\\\marid\\\\OneDrive\\\\Desktop\\\\Progetto SF\\\\Giraffa.txt\"\n",
    "}\n",
    "\n",
    "# Creazione del bot\n",
    "bot = AssemblyAssistantBot(manuals)\n",
    "\n",
    "# Menu a tendina per la selezione del prodotto\n",
    "product_dropdown = widgets.Dropdown(\n",
    "    options=[('Seleziona un prodotto', None)] + [(name, name) for name in manuals.keys()],\n",
    "    value=None,\n",
    "    description='Prodotto:',\n",
    ")\n",
    "\n",
    "# Campo di inserimento per la domanda\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Cosa vuoi fare?',\n",
    "    description='AssemblyBot:',\n",
    ")\n",
    "\n",
    "# Pulsante di invio\n",
    "submit_button = widgets.Button(\n",
    "    description='Invia',\n",
    "    button_style='primary',\n",
    ")\n",
    "submit_button.on_click(lambda _: on_submit())\n",
    "\n",
    "# Finestra di conversazione (modificata per visualizzare solo la risposta corrente)\n",
    "conversation_output = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='Conversazione:',\n",
    "    layout={'width': '100%', 'height': '400px'},\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_submit():\n",
    "    product = product_dropdown.value\n",
    "    query = query_input.value\n",
    "    if product and query:\n",
    "        response = bot.handle_query(product, query)\n",
    "        \n",
    "        # Unisci gli elementi della lista di risposte in una singola stringa\n",
    "        response_text = \"\\n\".join(response)\n",
    "        \n",
    "        # Mostra solo la risposta corrente, non le precedenti\n",
    "        conversation_output.value = f\"Tu: {query}\\n\\nBot: {response_text}\\n\"\n",
    "\n",
    "        query_input.value = ''\n",
    "\n",
    "\n",
    "# Layout dell'interfaccia\n",
    "display(widgets.VBox([product_dropdown, query_input, submit_button, conversation_output]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
